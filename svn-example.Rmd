---
title: SVN Example - Unidad 6
author: "Pío Sierra"
date: "10/5/2020"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document:
    toc: yes
params:
  data_file: colon2.csv
  num_genes: 2000
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include = FALSE}
if(!(require(kernlab)))
  install.packages("kernlab")
if(!(require(class)))
  install.packages("class")
if(!(require(gmodels)))
  install.packages("gmodels")
if(!(require(ROCR)))
  install.packages("ROCR")
if(!(require(caret)))
  install.packages("caret")
if(!(require(e1071)))
  install.packages("e1071")
```

***  
**Parámetros:**  
**data_file**: *ruta completa al archivo de datos.*  
**num_genes**: *número de genes expresados en los datos.* 
*** 


# El algoritmo SVN

El algoritmo SVN consiste en la creacción de hiperplanos en el espacio definido por los valores de las características que creen particiones en las que clasificar los datos.


Ventajas| Inconvenientes
-----|------
Pueden ser usados tanto para clasificación como para predicción numérica | Encontrar el mejor modelo exige probar con distintos kernels y parámetros.  
No se ve muy afectado por ruido en los datos ni es susceptible de sobreajuste. | El entrenamiento puede ser lento, sobre todo cuando se trata de datos con un gran número de características u observaciones.  
Puede resultar más sencillo de utilizar que las redes neuronales, debido a la existencia de varios algoritmos de SVN bien mantenidos. | El resultado es un complejo modelo de caja negra que resulta a menudo imposible de interpretar.  
Es bastante popular debido a su precisión y al haber resultado vencedor en varias competiciones de data mining. | -  



# 1 Lectura de datos

Procedemos a leer los datos. En los parámetros se deben especificar la ruta completa al archivo de datos así como el número de genes incluidos en las observaciones. El archivo debe tener una fila por observación, e incluir una columna "y" con los valores "n" o "t" según la observación proceda de un paciente normal o de un tumor.

```{r}
# Leemos los datos 
data <- read.csv(params$data_file)

```
# 2 Exploración y preparación de los datos

Comprobamos si los datos son completos y observamos la distribución de las medias de cada variable. También vemos la proporción de casos observados de cada tipo.
```{r}
# Comprobamos la estructura de los datos y si falta alguno
data[!complete.cases(data)]
hist(apply(data[,1:params$num_genes],2,mean))
table(data$y)

```
```{r}
# Convertimos y en un factor
data$y <- as.factor(data$y)


```
```{r}

set.seed(12345)
smp_size <-  nrow(data) - trunc(nrow(data) / 3)
train_ind <-
  sample(seq_len(nrow(data)), size = smp_size)

data_train <- data[train_ind,]
data_test <- data[-train_ind,]

```

# 3 Entrenamiento del modelo

```{r}
m <- ksvm(y ~ ., data = data_train, kernel = "vanilladot")
```
```{r}
p <- predict(m, data_test)
 
```
# 4 Evaluación del rendimiento del modelo

```{r}

confusionMatrix(p, data_test$y, "t")
```

# 5 Optimización del rendimiento

```{r}
# Procedemos a normalizar los datos
# normalize <- function(x) {
# return ((x - min(x)) / (max(x) - min(x)))
# }
# data <- apply(data[,1:params$num_genes],2,normalize)
```