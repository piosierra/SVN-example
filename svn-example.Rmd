---
title: SVN Example - Unidad 6
author: "Pío Sierra"
date: "10/5/2020"
output:
  pdf_document:
    toc: yes
  html_document:
    df_print: paged
    toc: yes
params:
  data_file: colon2.csv
  num_genes: 2000
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include = FALSE}
if(!(require(kernlab)))
  install.packages("kernlab")
if(!(require(class)))
  install.packages("class")
if(!(require(gmodels)))
  install.packages("gmodels")
if(!(require(ROCR)))
  install.packages("ROCR")
if(!(require(caret)))
  install.packages("caret")
if(!(require(e1071)))
  install.packages("e1071")
```

***  
**Parámetros:**  
**data_file**: *ruta completa al archivo de datos.*  
**num_genes**: *número de genes expresados en los datos.*  

*** 


# El algoritmo SVN

El algoritmo SVN consiste en la creacción de hiperplanos en el espacio definido por los valores de las características que creen particiones en las que clasificar los datos.  

Ventajas| Inconvenientes
-----|------
Pueden ser usados tanto para clasificación como para predicción numérica | Encontrar el mejor modelo exige probar con distintos kernels y parámetros.  
No se ve muy afectado por ruido en los datos ni es susceptible de sobreajuste. | El entrenamiento puede ser lento, sobre todo cuando se trata de datos con un gran número de características u observaciones.  
Puede resultar más sencillo de utilizar que las redes neuronales, debido a la existencia de varios algoritmos de SVN bien mantenidos. | El resultado es un complejo modelo de caja negra que resulta a menudo imposible de interpretar.  
Es bastante popular debido a su precisión y al haber resultado vencedor en varias competiciones de data mining. | -  

El algoritmo consiste en identificar los Support Vectors, las observaciones de cada clase que están más cerca del Hyperplano de margen máximo (MMH). Se demuestra que estos puntos bastan para definir el hiperplano. Para el caso de datos que no son linealmente separables, se utiliza una variable slack que define un margen al otro lado del hiperplano al que es aceptable tener alguna observación.  

Otra ventaja de SVN es la posiblidad de añadir más dimensiones a las observaciones para así hacer visibles (lineales) relaciones entre las observaciones.  

# 1 Lectura de datos

Procedemos a leer los datos. En los parámetros se deben especificar la ruta completa al archivo de datos así como el número de genes incluidos en las observaciones. El archivo debe tener una fila por observación, e incluir una columna "y" con los valores "n" o "t" según la observación proceda de un paciente normal o de un tumor.

```{r}
# Leemos los datos 
data <- read.csv(params$data_file)

```
# 2 Exploración y preparación de los datos

Comprobamos si los datos son completos y observamos la distribución de las medias de cada variable. También vemos la proporción de casos observados de cada tipo.
```{r}
# Comprobamos la estructura de los datos y si falta alguno
data[!complete.cases(data)]
hist(apply(data[,1:params$num_genes],2,mean), 
     main = "Histograma de los valores médios de cada una de las variables")
table(data$y)

```
Convertimos la variable del resultado en un factor.
```{r}
# Convertimos y en un factor
data$y <- as.factor(data$y)

```
A continuación creamos los conjuntos de entrenamiento y prueba, respectivamente 2/3 y 1/3 del original.
```{r}
set.seed(12345)
smp_size <-  nrow(data) - trunc(nrow(data) / 3)
train_ind <-
  sample(seq_len(nrow(data)), size = smp_size)

data_train <- data[train_ind,]
data_test <- data[-train_ind,]
```

# 3 Entrenamiento del modelo

Procedemos a entrenar el modelo usando `kernel = "vanilladot`, el kernel para un modelo lineal 
```{r}
ml <- ksvm(y ~ ., data = data_train, kernel = "vanilladot")

```
Y generamos las predicciones para ambos modelos.  

```{r}
pm <- predict(ml, data_test)

```
# 4 Evaluación del rendimiento del modelo

```{r}

confusionMatrix(pm, data_test$y, positive = "t")

```

# 5 Optimización del rendimiento

Como opción de optimización procedemos a utilizar ahora `kernel = "rbfdot` para el Radial Basis kernel (gausiano), y transformar los datos aumentando las dimensiones en busca de un mejor rendimiento.

```{r}
mg <- ksvm(y ~ ., data = data_train, kernel = "rbfdot")
pg <- predict(mg, data_test)
confusionMatrix(pg, data_test$y, positive = "t")
```

